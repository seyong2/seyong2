---
layout: post
title: P-hacking 
subtitle: What it is and how to avoid it
gh-repo: seyong2
gh-badge: [star, fork, follow]
tags: [Statistics, Machine Learning, P-hacking]
comments: true
---

*This post is based on the videos by [StatQuest with Josh Starmer](https://www.youtube.com/@statquest/featured).* 

Have you heard of p-hacking before? What do you think p-hacking tells you? 
What about data dredging, data fishing, data snooping, or data butchery? All these terms refer to the same thing, a situation in which data analysis is abused to find out results that appear statistically significant even if they actually aren’t in reality. These kinds of results that deceive us are called false positives. 

Let’s examine an example where p-hacking may happen. It is known that humans have between 20,000 and 25,000 genes and identical twins share a significant amount of similar gene materials. Then, suppose we take two men at random and compare their 20,000 genes, for example, to see whether they are identical twins or not. Assuming that the significance level is 0.05, 5 percent of the time we are doing 20,000 tests, we will end up with p-values less than 0.05 and might draw an erroneous conclusion that there is a statistically significant difference between the two men and that they are not identical twins. And the more tests we do, the more false positives the tests will result in. This is called the multiple testing problem and one popular method to avoid this is to use a false discovery rate (FDR).

Before going deeper into FDR, we need to know what p-values look like when the null hypothesis is true and when it is not. Under the null hypothesis that the two men share the same DNA, the p-values would be uniformly distributed between 0 and 1. At first, I could not understand why this was the case that the distribution of the p-values is uniform. However, if we remind ourselves of the definition of p-value, it is the probability, under the null hypothesis, of obtaining a result equal to or more extreme than what was actually observed. Then, if we count the number of the test statistics that belong to each range of p-values, and plot a histogram, the distribution of the p-values across the different bins will be even.

![p_value_histogram_unifrom](https://github.com/seyong2/seyong2.github.io/blob/master/assets/img/figures_p_hacking/p_values_uniform.png?raw=true)

On the contrary, when the alternative hypothesis is true, that is, when the two samples are from two different populations, the distribution of the p-values will be right skewed because most of the p-values would be less than 0.05. In this case, the p-values that are greater than the threshold of 0.05 are false negatives and are the result of the two distributions being somewhat overlapped. The number of false negatives can be reduced by increasing the sample size.

![p_value_histogram_right_skewed](https://github.com/seyong2/seyong2.github.io/blob/master/assets/img/figures_p_hacking/p_values_uniform_0_0.5.png?raw=true)

In order to isolate the true positives from the false positives, one way is to order p-values from lowest to highest and to consider only a part of the values, starting with the smallest. Since when the null is not true, the p-values within the bin between 0 and 0.05 are much more skewed than those under the null hypothesis, which are evenly distributed.

![p_value_histogram_unifrom_0_0.5](https://github.com/seyong2/seyong2.github.io/blob/master/assets/img/figures_p_hacking/p_values_right_skewed_0_0.5.png?raw=true)

![p_value_histogram_right_skewed](https://github.com/seyong2/seyong2.github.io/blob/master/assets/img/figures_p_hacking/p_values_right_skewed.png?raw=true)

The Bengamini-Hochberg method is a numerical version of the above method. It adjusts p-values by making them slightly larger, which helps limit the number of false positives. The steps are as follows: 

1. Order the p-values from smallest to largest.
2. Rank the p-values.
3. The largest FDR-adjusted p-value and the largest p-value are the same.
4. The next largest adjusted p-value is the smaller of two options;
$min(the previously adjusted p-value, the current p-value\times\frac{total # of p-values}{p-value rank}

Now, we’ll have a look at another form of p-hacking and a way to avoid it. Suppose that as before, we are doing a test to see whether the two samples come from the same distribution or not. If we have a p-value that is close to 0.05 but larger than 0.05, we might want to add more data because it is highly likely that it results in a p-value less than 0.05 and we could conclude that there is a statistically significant difference in the two samples. However, this cannot be done because there is a high probability that this brings about a false positive and we are calculating the p-value twice to make a decision. To prevent us from making this mistake, the appropriate sample size should be determined before doing the experiment using power analysis. This will tell us how many measurements we are required to have to correctly reject the null hypothesis with high confidence. 

As the name suggests, power is the probability that we will correctly reject the null hypothesis. It is affected mainly by two factors; sample size and how much overlap the two distributions have. For a given number of measurements for each sample, the more overlap the two populations have, the lower the power is because it is more difficult to distinguish one from another. On the other hand, even when the two distributions overlap, if the sample size is sufficiently large, we can have high power because more measurements lead to better estimations of population parameters, such as population mean. Therefore, the parameter values estimated by a large sample size for the two populations will overlap less, which in turn will increase the power. These results apply to any underlying distribution by the central limit theorem that says that it doesn't matter what distribution you start with if you collect enough samples from those distributions, then the means will be normally distributed. Therefore, having a sample that is sufficiently large will allow us to have a large power and to confidently make a good decision regardless of the p-value.

Then how is a power analysis done? First, we need to decide how much power we want from a value 0 to 1 and 0.8 is a typically chosen value. Then, we also need to determine the threshold for significance from 0 to 1, but a very common value is 0.05. Next, we should estimate how much distributions overlap. Overlap is affected by the distance between the population means and standard deviations. To combine these factors into a single metric, we compute an effect size ($d$), which is equal to the estimated difference in the means / pooled estimated standard deviations. One of the simplest ways to pool the estimated standard deviations is the squared root of half of the sum of the standard deviations. Finally, we search for a statistics power calculator on the Internet that computes for us the minimum number of subjects for the power we set in the first place.
