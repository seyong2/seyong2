---
layout: post
title: Understandong DBSCAN
subtitle: A Clustering Technique You'd Better Know
gh-repo: seyong2
gh-badge: [star, fork, follow]
tags: [Statistics, Machine Learning, DBSCAN, Silhouette Score, K-Means, Hierarchical Clustering, K-Distance Graph]
comments: true
---

*This post is based on [StatQuest with Josh Starmerâ€™s video about DBSCAN](https://www.youtube.com/watch?v=RDZUdRSDOok) and [How to Master the Popular DBSCAN Clustering Algorithm for Machine Learning by Analytics Vidhya](https://www.analyticsvidhya.com/blog/2020/09/how-dbscan-clustering-works/).*

DBSCAN, an acronym for Density-Based Spatial Clustering of Applications with Noise, is an algorithm designed to perform clustering by considering the data's density rather than its explicit features, as evident from its name. Clustering, an unsupervised machine learning technique, aims to categorize data based on shared characteristics. The commonly used clustering methods include K-means and hierarchical clustering algorithms. Nonetheless, these algorithms possess limitations that drive us towards alternative clustering methods, such as DBSCAN, which I will discuss in this article. Traditional clustering algorithms struggle to identify clusters with irregular shapes. To illustrate this point, consider a dataset resembling two intertwined half circles. The plot below reveals two potential clusters with some accompanying noise.

![data](https://github.com/seyong2/seyong2.github.io/blob/master/assets/img/figures_dbscan/data.png?raw=true)

What happens if we attempt to group the data into two clusters using K-means and hierarchical clustering techniques? The following plots demonstrate the outcomes generated by these two algorithms. It is evident that both algorithms fall short of identifying the expected two clusters accurately. Certain data points belonging to one circle are mistakenly classified as if they originated from the other circle. Moreover, the algorithms fail to detect outliers, as all points are categorized solely as purple or yellow. Observing the shortcomings of K-means and hierarchical clustering algorithms with data of arbitrary shapes, it is not difficult to envision their performance when applied to high-dimensional data.

![k-means](https://github.com/seyong2/seyong2.github.io/blob/master/assets/img/figures_dbscan/kmeans.png?raw=true)

![hierarchical](https://github.com/seyong2/seyong2.github.io/blob/master/assets/img/figures_dbscan/hierarchical.png?raw=true)

DBSCAN, as mentioned earlier, offers an alternative to the conventional clustering methods we've discussed. It constructs clusters based on the densities of the data points. Consequently, tightly packed data points are grouped into clusters, while those residing in low-density regions are identified as outliers. An additional advantage of DBSCAN is that it doesn't require the user to predefine the number of clusters, unlike methods like K-means that rely on specifying the number of centroids. Then, the clustering process can be summarized as follows:

1. Commencing with the unclustered data, we determine the proximity of each point by enclosing it within a circle. The distance between points is commonly measured using Euclidean distance, although other distance metrics can be considered. The circle's size, represented by its radius ($\epsilon$), is a hyperparameter that requires predefining. To determine an optimal value for $\epsilon$, a typical approach involves plotting the K-distance graph and identifying the point of maximum curvature.

2. We assign a categorization to each data point based on the number of nearby data points, including itself. If a point has at least $x$ close points, it is classified as a core point; otherwise, it is deemed a non-core (or border) point. Similar to $\epsilon$, the number of close points for a core point is a hyperparameter. It should be set to at least the number of dimensions in the data plus one, although a common rule of thumb is to double the dimensions. Incorporating domain knowledge can also aid in determining an optimal value.

3. We randomly select a core point and assign it to the first cluster. Subsequently, we include all other core points adjacent to the first cluster within the same cluster. This process of cluster extension continues until no additional core points can be added. It is important to note that border points are not considered during this step.

4. After completing the previous step, we incorporate all non-core points that are in close proximity to the core points into the first cluster.

5. Next, we select another core point, not yet assigned to any cluster, and form a second cluster. We repeat the same process of extending the cluster as described earlier.

6. Finally, any remaining non-core points that do not belong to any of the clusters are classified as noise.

Based on the aforementioned approach, I conducted clustering using DBSCAN on the same dataset. Initially, I set the minimum number of points in a neighborhood for a point to be classified as a core point as 4, considering the data's two-dimensional nature. Subsequently, I examined the K-distance graph with a distance parameter of 4 to identify the appropriate radius for the circle. Upon analyzing the graph, it appeared that the curvature was highest at $\epsilon$ equal to 0.05. Based on this observation, I proceeded to fit the model and visualize the results, hoping to observe how DBSCAN clustered the data. Unfortunately, the outcome was rather underwhelming as it failed to effectively differentiate one cluster from the other. Surprisingly, the model identified more than two clusters, exceeding my initial expectation. Consequently, I decided to adopt an alternative approach for determining the value of $\epsilon$, which involved utilizing the silhouette score.

![k-distance_graph](https://github.com/seyong2/seyong2.github.io/blob/master/assets/img/figures_dbscan/k_distance_graph.png?raw=true)

![dbscan1](https://github.com/seyong2/seyong2.github.io/blob/master/assets/img/figures_dbscan/dbscan_eps_0.05.png?raw=true)

The silhouette score serves as a metric for evaluating the quality of a clustering technique. It calculates the average distance from each sample to other points within the same cluster ($a$) and the average distance from the same sample to points belonging to the nearest cluster ($b$). For instance, the silhouette score for sample $i$ can be defined as $\frac{b(i)-a(i)}{\max(a(i),b(i))}$. By computing the mean score across all data points, we can assess the effectiveness of the clustering. Ideally, a silhouette score of 1 indicates that the mean distance to the nearest cluster is maximized, while the mean distance within the same cluster approaches 0.

To explore the impact of different values of $\epsilon$ (with a minimum core point requirement of 4), I calculated the silhouette scores across a range of values (from 0.05 to 0.2 with a step size of 0.01). The table presented below showcases the corresponding silhouette scores for each $\epsilon$ value. Based on this analysis, it is determined that the silhouette score reaches its maximum value of 0.31 when the radius of the circle is set to 0.11. Consequently, when fitting the model again with $\epsilon$ equal to 0.11 and visualizing the results, it becomes evident that the outcome appears promising compared to when $\epsilon$ was set to 0.05. The model successfully identifies two clusters, represented by the yellow and green points, while the purple points are classified as outliers that do not belong to either cluster.

However, it remains unclear why the K-distance graph led to a value of $\epsilon$ that resulted in poor model performance. If you have any insights or ideas regarding this matter, please feel free to share them with me.

| $\epsilon$ | Silhouette Score |
| :---: | :---: |
| 0.05 | -0.71 |
| 0.06 | -0.58 |
| 0.07 | -0.30 |
| 0.08 | -0.13 |
| 0.09 | -0.04 |
| 0.10 | 0.05 |
| 0.11 | 0.31 |
| 0.12 | 0.15 |
| 0.13 | 0.16 |
| 0.14 | 0.17 |
| 0.15 | 0.22 |
| 0.16 | 0.21 |
| 0.17 | 0.24 |
| 0.18 | 0.25 |
| 0.19 | 0.17 |
| 0.20 | 0.17 |

![dbscan2](https://github.com/seyong2/seyong2.github.io/blob/master/assets/img/figures_dbscan/dbscan_eps_0.11.png?raw=true)


