---
layout: post
title: CatBoost
subtitle: Part 2 - Comparison of different encoding methods
gh-repo: seyong2
gh-badge: [star, fork, follow]
tags: [Statistics, Machine Learning, CatBoost, Encoding, One-Hot Encoding, Label Encoding, Target Encoding]
comments: true
---

This post is dedicated to comparing the encoding methods we saw in the previous post. If you didn't, check out [CatBoost Part 1 - How CatBoost deals with categorical features](https://seyong2.github.io/2023-05-21-catboost1/). To this end, we are going to use the [Data Science Salaries 2023](https://www.kaggle.com/datasets/arnabchaki/data-science-salaries-2023) dataset and predict the gross salary of a data scientist based on different features. The dataset has 11 columns in total and the description of the columns is given in the following table.

| Column | Description |
| :---: | :---: |
| `work_year` | The year the salary was paid |
| `experience_level` | The experience level in the job during the year | 
| `employment_type` | The type of employment for the role |
| `job_title` | The role worked in during the year |
| `salary` | The total gross salary amount paid |
| `salary_currency` | The currency of the salary paid as an ISO 4217 currency code |
| `salary_in_usd` | The salary in USD|
| `employee_residence` | Employee's primary country of residence in during the work year as an ISO 3166 country code |
| `remote_ratio` | The overall amount of work done remotely | 
| `company_location` | The country of the employer's main office or contracting branch |
| `company_size` | The median number of people that worked for the company during the year |

Without furtherado, let's get started by importing the data and have a look at the first five rows.

```
import pandas as pd
df = pd.read_csv('/content/drive/MyDrive/ds_salaries.csv')
df.head()
```

| `work_year` | `experience_level` | `employment_type` | `job_title` | `salary` | `salary_currency` | `salary_in_usd`	| `employee_residence` | `remote_ratio` | `company_location` | `company_size` |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 2023 | SE | FT | Principal Data Scientist | 80000	| EUR | 85847 | ES | 100 | ES | L |
| 2023 | MI | CT | ML Engineer | 30000	| USD | 30000 | US | 100 | US | S |
| 2023 | MI | CT | ML Engineer | 25500	| USD | 25500 | US | 100 | US | S |
| 2023 | SE | FT | Data Scientist | 175000	| USD | 175000 | CA | 100 | CA | M |
| 2023 | SE | FT | Data Scientist | 120000	| USD | 120000 | CA | 100 | CA | M |

The data have 11 columns in total and the type of each column seems to be either numerical or categorical. For example, the first five values of the column `employment_type` are not numbers but strings. Indeed, the types of the columns, as can be seen below, are either `int64` or `object`.

```
df.dtypes
```
| Column | Type |
| :---: | :---: |
| work_year | int64 |
| experience_level | object |
| employment_type | object |
| job_title | object |
| salary | int64 | 
| salary_currency | object |
| salary_in_usd | int64 |
| employee_residence | object |
| remote_ratio | int64 |
| company_location | object |
| company_size | object |
