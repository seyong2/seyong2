---
layout: post
title: CatBoost
subtitle: 
gh-repo: seyong2
gh-badge: [star, fork, follow]
tags: [machine learning, catboost, encoding, cosine similarity]
comments: true
---

I was scrolling through the other night the YouTube page of [StatQuest with Josh Starmer](https://www.youtube.com/@statquest) and I saw that there waere videos uploaded recently about CatBoost, which is a Machine Learning algorithm that I have never heard of. Thus, I decided to watch the series (part 1 and 2) and the videos related to the series that may help understand the algorithm. And the aim of this post is to give a summary of what I learned so that I can come back and refresh this concept whenever necessary. Then, without further ado, let's get started!

CatBoost, a.k.a. Categorical Boosting, is an machine learning algorithm which is very similar to Gradient Boost and XGBoost. As the name suggests, CatBoost has a unique way of handling categorical variables. As you might already know, a lot of machine learning methods do not work well with categorical features. Therefore, before we build a model, we often convert categorical variables into numerical values and there are various ways to do it.

One popular encoding method is **One-Hot encoding**, that creates new binary columns for each option of a categorical feature when it has three or more options. For example, let's suppose that we want to predict whether a person likes to drink coffee or not based on the country this person is from (Spain, South Korea, and the United States). Then, since there are three unique country values in this column `Country`, the data will have three binary features denpending on the nationality of each person once one-hot encoding is done.

Before Encoding...

| Country | LikeCoffee |
| :---: | :---: |
| Spain | Yes |
| South Korea | Yes |
| The United States | No |
| South Korea | Yes |
| The United States | Yes |
| Spain | No |

After One-Hot Encoding...

| Country_Spain | Country_South_Korea| Country_The_United_States | LikeCoffee |
| :---: | :---: | :---: | :---: |
| 1 | 0 | 0 | Yes |
| 0 | 1 | 0 | Yes |
| 0 | 0 | 1 | No |
| 0 | 1 | 0 | Yes |
| 0 | 0 | 1 | Yes |
| 1 | 0 | 0 | No |

The major disadvantage of this encoding scheme is that the model may not work well in case a categorical variable has a tons of options as we will have the same number of new columns after the encoding. Thus, in this case, we can turn to another method called **Label Encoding**, which assigns a random number to each option from low to high. However, because the numbers are just arbitrary, some machine learning algorithms will treat the numbers as if the order might mean something, and that can cause problems.

After Label Encoding...

| Country | LikeCoffee |
| :---: | :---: |
| 0 | Yes |
| 1 | Yes |
| 2 | No |
| 1 | Yes |
| 2 | Yes |
| 0 | No |

There is another common encoding method: **Target Encoding**. This encoding scheme makes use of target variable, the variable that we want to predict, to transform each option of a categorical feature into a number. In general, we calcualtes a weighted mean, combining the mean for a particular option with the overall mean of the target. And this allows us to have the best guess as a value close to the overall mean for an option when there are little data for that option.
$ Weighted Mean = \frac{n\times Option Mean + m\times Overall Mean}{n+m}$
with $n$ equal to the weight for Option Mean (the number of rows) and $m$ is the weight for Overall Mean (hyperparameter). In the example that we have, if we set $m$ equal to 1, meaning that at least 2 data are needed so that the Option Mean becomes more important than the Overall Mean, the data, after the target encoding, will look like below...

After Target Encoding...

| Country | LikeCoffee |
| :---: | :---: |
| 0.56 | Yes |
| 0.89 | Yes |
| 0.56 | No |
| 0.89 | Yes |
| 0.56 | Yes |
| 0.56 | No |

However, since this method uses the target variable to change the values of categorical features, this results in data leakage (Josh Stamer defines data leakage as a situation in which the encoding of each data row is affected by its corresponding target value in this series of CatBoost), which leads to models that perform well on training data, but not great on testing data (overfitting). To mitigate data leakage, we can use one of the popular alternatives, **K-Fold Target Encoding**. For this, training data is divided into K equal sized subsets and we do the target encoding on a subset using the other remaining subsets. Thus, when K is equal to the number of rows of data, K-Fold target encoding is the same as Leave-One-Out target encoding. Suppose, in our example, that we want to do 3-Fold Target Encoding. Then, we would divide the data as below.

Subset 1

| Country | LikeCoffee |
| :---: | :---: |
| Spain | Yes |
| South Korea | Yes |

Subset 2

| Country | LikeCoffee |
| :---: | :---: |
| The United States | No |
| South Korea | Yes |

Subset 3

| Country | LikeCoffee |
| :---: | :---: |
| The United States | Yes |
| Spain | No |

To target encode subset 1, first, we use subset 2 and 3. In other words, for example, the value that will replace the option *Spain* in subset 1, we look at the other subsets. Then, using the weighted mean equation, and assuming $m$ is equal to 1, we will plug in $\frac{1\times 0 + 1\times 0.5}{1+1} = 0.25$ for the row in subset 1 with *Spain*. Therefore, the data with the 3-Fold target encoding will look like as follows:

| Country | LikeCoffee |
| :---: | :---: |
| 0.25 | Yes |
| 0.75 | Yes |
| 0.875 | No |
| 0.875 | Yes |
| 0.375 | Yes |
| 0.875 | No |

It seems that K-Fold target encoding scheme helps reduce the risk of data leakage, but the authors of CatBoost algorithm thought of a scenario where this does not work. That scenario is illustrated as in the following table.

| Country | LikeCoffee |
| :---: | :---: |
| Spain | Yes |
| Spain | Yes |
| Spain | No |
| Spain | Yes |
| Spain | Yes |
| Spain | No |

We see that the column $Country$ has only one value, *Spain*. Then, if we apply Leave-One-Out target encoding to the data,...

| Country | LikeCoffee |
| :---: | :---: |
| 0.4 | Yes |
| 0.4 | Yes |
| 0.8 | No |
| 0.4 | Yes |
| 0.4 | Yes |
| 0.8 | No |

In other words, the people who like drinking coffee have $Country$ equal to 0.4, and those who do not like coffee have 0.8, meaning that we have data leakage because depending on $Country$, we are able to make perfect predictions on $LikeCoffee$.  

Josh thinks that cases like this is not common to observe and cannot happen because we do not usually include variables that have only one value since they are useless for any kind of model. Furthermore, even if we include such features, it is just easier to encode them by converting the option to a single value...
