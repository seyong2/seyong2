---
layout: post
title: P-hacking 
subtitle: What it is and how to avoid it
gh-repo: seyong2
gh-badge: [star, fork, follow]
tags: [Statistics, Machine Learning, P-hacking]
comments: true
---

*This post is based on the videos by [StatQuest with Josh Starmer](https://www.youtube.com/@statquest/featured).* 

Are you familiar with the concept of p-hacking? What insights do you think p-hacking provides? Similarly, have you come across terms like data dredging, data fishing, data snooping, or data butchery? These terms all encompass the same idea, which involves misusing data analysis to uncover results that seem statistically significant, even if they aren't truly meaningful. Such misleading outcomes are commonly referred to as false positives.

Let's explore an example where p-hacking might occur. It is established that humans possess around 20,000 to 25,000 genes, and identical twins share a significant amount of genetic material. Now, let's suppose we randomly select two men and compare their 20,000 genes to determine if they are identical twins. Assuming a significance level of 0.05, if we conduct 20,000 tests, approximately 5 percent of the time (or 1,000 tests), we might obtain p-values below 0.05. This could lead to an incorrect conclusion that there is a statistically significant difference between the two men, suggesting they are not identical twins. Furthermore, as the number of tests increases, the number of false positives also increases. This situation is known as the multiple testing problem, and one commonly used approach to address it is employing a false discovery rate (FDR).

Before delving into FDR, it is important to understand how p-values behave under different scenarios: when the null hypothesis is true and when it is not. Under the null hypothesis that the two men share the same DNA, the distribution of p-values would be uniformly spread between 0 and 1. At first, it seemed puzzling to my why the distribution of p-values would follow this uniform pattern. However, considering the definition of a p-value as the probability, under the null hypothesis, of obtaining a result equal to or more extreme than the observed outcome, it became clearer. By categorizing the test statistics into various p-value ranges and creating a histogram, the distribution of p-values across these bins will appear even.

![p_value_histogram_unifrom](https://github.com/seyong2/seyong2.github.io/blob/master/assets/img/figures_p_hacking/p_values_uniform.png?raw=true)

Conversely, when the alternative hypothesis is true, meaning that the two samples originate from distinct populations, the distribution of p-values will exhibit right-skewness. This is due to the majority of p-values being smaller than 0.05. In such instances, the p-values surpassing the 0.05 threshold represent false negatives, arising from a certain degree of overlap between the two distributions. By enlarging the sample size, it is possible to decrease the count of false negatives.

![p_value_histogram_right_skewed](https://github.com/seyong2/seyong2.github.io/blob/master/assets/img/figures_p_hacking/p_values_uniform_0_0.5.png?raw=true)

To distinguish the true positives from the false positives, one approach is to arrange the p-values in ascending order and focus solely on a specific range, beginning with the smallest values. This is because when the null hypothesis is false, the p-values within the 0 to 0.05 range exhibit a significantly higher degree of skewness compared to those under the null hypothesis, which are evenly distributed.

![p_value_histogram_unifrom_0_0.5](https://github.com/seyong2/seyong2.github.io/blob/master/assets/img/figures_p_hacking/p_values_right_skewed_0_0.5.png?raw=true)

![p_value_histogram_right_skewed](https://github.com/seyong2/seyong2.github.io/blob/master/assets/img/figures_p_hacking/p_values_right_skewed.png?raw=true)

The Bengamini-Hochberg method is a computational approach based on the aforementioned method. It involves modifying p-values to slightly increase their values, thereby mitigating the occurrence of false positives. The procedure can be outlined as follows:

1. Arrange the p-values in ascending order.
2. Assign ranks to the p-values.
3. The largest FDR-adjusted p-value is equal to the largest observed p-value.
4. Calculate the next adjusted p-value as the smaller value between the previously adjusted p-value and the current p-value multiplied by the total number of p-values divided by the rank of the p-value.

By employing the Bengamini-Hochberg method, the adjusted p-values can effectively control the false positive rate, allowing for a more accurate analysis of statistical significance.

Now, let's explore another form of p-hacking and a method to prevent it. Consider a scenario similar to before, where we are conducting a test to determine whether two samples originate from the same distribution. If we obtain a p-value that is close to 0.05 but greater than 0.05, there might be a temptation to collect additional data, as it is highly likely that this would yield a p-value below 0.05. This could lead to the conclusion that there is a statistically significant difference between the two samples. However, this practice is not advisable since it significantly increases the risk of a false positive, as we would essentially be calculating the p-value twice to make a decision. To avoid this mistake, it is crucial to determine the appropriate sample size before conducting the experiment, using a technique called power analysis. Power analysis helps determine the number of measurements required to confidently reject the null hypothesis with a high level of confidence.

As the name suggests, power refers to the probability of correctly rejecting the null hypothesis. It is primarily influenced by two factors: the sample size and the degree of overlap between the two distributions. For a fixed number of measurements in each sample, if the two populations have a greater degree of overlap, the power decreases, as distinguishing between the two becomes more challenging. Conversely, even when there is overlap between the two distributions, a sufficiently large sample size can yield high power. This is because a larger number of measurements enables better estimations of population parameters, such as the population mean. Consequently, the parameter values estimated from a larger sample size for the two populations will exhibit less overlap, resulting in increased power. These findings hold true for any underlying distribution, thanks to the central limit theorem, which states that regardless of the starting distribution, if a sufficiently large sample is collected, the estimated means will follow a normal distribution. Therefore, having a sample size that is adequately large allows us to achieve high power and make confident decisions regardless of the p-value.

![overlap](https://github.com/seyong2/seyong2.github.io/blob/master/assets/img/figures_p_hacking/overlap.png?raw=true)
![mean estimation](https://github.com/seyong2/seyong2.github.io/blob/master/assets/img/figures_p_hacking/mean estimation.png?raw=true)

So, how is power analysis conducted? Firstly, we need to determine the desired level of power, typically set between 0 and 1, with 0.8 being a commonly chosen value. Next, we establish the significance threshold, which is often set to 0.05. Then, we estimate the extent of overlap between the distributions. Overlap is influenced by factors such as the distance between the population means and the standard deviations. To combine these factors into a single measure, we calculate an effect size ($d$), which represents the estimated difference in means divided by the pooled estimated standard deviations. One simple approach to pooling the estimated standard deviations is to take the square root of half the sum of the standard deviations. Finally, we can search for an online statistical power calculator that determines the minimum number of subjects needed to achieve the desired power level established earlier.

By conducting a power analysis prior to conducting experiments, we can ensure that our study has an appropriate sample size, minimizing the risk of false positives and enabling confident decision-making based on statistical results.
