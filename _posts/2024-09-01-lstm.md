---
layout: post
title: From Theory to Application: LSTM in Action
subtitle: Exploring the Capabilities of Long Short-Term Memory Networks
gh-repo: seyong2
gh-badge: [star, fork, follow]
tags: [Artificial Intelligence, Machine Learning, Data Science, Neural Network, Deep Learning, Sequential Modeling, RNN, LSTM]
comments: true
---

# Recurrent Neural Networks

Imagine we want to build a binary classification model to predict whether a phrase contains sarcasm. For instance, consider these two sentences: "You have just broken my favorite dish, great job" and "Thanks for helping out, great job". Although both sentences include the phrase "great job", the first is sarcastic, while the second is sincere. However, traditional neural networks may struggle to distinguish between the two because they don't account for the order of the words. Without considering word sequence, the model might incorrectly classify both sentences as sincere. 

Recurrent Neural Networks (RNNs) can effectively address this challenge. RNNs are a type of artificial neural network specifically designed for processing sequential data, such as time series, text, and audio, where the order of the data points is crucial. RNNs can retain information from previous inputs in a sequence through loops in their structure.

![RNN](https://github.com/user-attachments/assets/b032ffc2-b193-45e7-a085-729a259ba9eb)

In the diagram above, a portion of the neural network (represented by the gray box in the middle, labeled as $h_t$) receives an input $x_t$ and produces an output $y_t$ at time step $t$. The hidden state $h_t$ can be expressed as follows:

$$ h_t = tanh(w_x \times x_t + w_h \times h_{t-1} + b) $$

At first glance, it might not be immediately clear why $h_t$ depends on $h_{t-1}$. However, by examining the unrolled version of the network, we can better understand the structure of RNNs. A recurrent neural network can be visualized as multiple copies of the same network, each passing information to the next step in the sequence. This design means that we only have three parameters- $w_x$, $w_h$, and $b$- which are shared across all copies of the network.

![RNN-Unrolled](https://github.com/user-attachments/assets/cb2f3c4f-b476-4a3e-b495-4c5bd01ba8b8)

# Gradient Vanishing and Exploding

While RNNs are well-suited for processing sequential data effectively due to their structure, there are scenarios where they may struggle. Consider a situation where we have a longer text, and we want to predict the final word of the text "I went grocery shopping and bought a steak. Then, I met up with my friend, and we had lunch together... When I came back home, I prepared dinner with what I bought earlier, the *steak*". Recent context suggests that the next word is likely a type of food, but to pinpoint exactly which food, we need to recall the mention of steak from much earlier in the text. As the gap between the relevant information and the point where it's needed increases, RNNs struggle to make the connection. This difficulty in learning long-term dependencies is a significant limitation of RNNs. Let's explore this issue with a simple example.
