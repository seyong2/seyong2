---
layout: post
title: CatBoost
subtitle: 
gh-repo: seyong2
gh-badge: [star, fork, follow]
tags: [machine learning, catboost, encoding, cosine similarity]
comments: true
---

I was scrolling through the other night the the YouTube page of [StatQuest with Josh Starmer](https://www.youtube.com/@statquest) and I saw that there was a video uploaded recently about CatBoost, which is a Machine Learning algorithm that I have never heard of. Thus, I decided to watch the series (part 1 and 2) and the videos related to the series that may help understand the algorithm. And the aim of this post is to give a summary of what I learned for me so that I can come back and refresh this concept whenever necessary. So, without further ado, let's get started!

CatBoost, a.k.a. Categorical Boosting, is an machine learning algorithm which is very similar to Gradient Boost and XGBoost. As the name suggests, CatBoost has a unique way of handling categorical variables. As you might already know, a lot of machine learning methods do not work well with categorical features. Therefore, before we build a model, we often convert categorical variables into numerical values and there are various ways to do it.

One popular encoding method is one-hot encoding, that creates new binary columns for each option of a categorical feature when it has three or more options. For example, let's suppose that we want to predict whether a person likes to drink coffee or not based on the country this person is from (Spain, South Korea, and the United States). Then, since there are three unique country values in this column `Country`, the data will have three binary features denpending on the nationality of each person once one-hot encoding is done.

Before Encoding...

| Country | LikeCoffee |
| :---: | :---: |
| Spain | Yes |
| South Korea | Yes |
| The United States | No |
| South Korea | Yes |
| The United States | Yes |
| Spain | No |

After One-Hot Encoding...

| Country_Spain | Country_South_Korea| Country_The_United_States | LikeCoffee |
| :---: | :---: | :---: | :---: |
| 1 | 0 | 0 | Yes |
| 0 | 1 | 0 | Yes |
| 0 | 0 | 1 | No |
| 0 | 1 | 0 | Yes |
| 0 | 0 | 1 | Yes |
| 1 | 0 | 0 | No |

The major disadvantage of this encoding scheme is that the model may not work well in case there are tons of options for a categorical variable as we will have the same number of new columns after the encoding. Thus, in this case, we can turn to another method called Label Encoding, which assigns a random number to each option from low to high. However, because the numbers are just arbitrary, some machine learning algorithms will treat the numbers as if the order might mean something, and that can cause problems.

After Label Encoding...

| Country | LikeCoffee |
| :---: | :---: |
| 0 | Yes |
| 1 | Yes |
| 2 | No |
| 1 | Yes |
| 2 | Yes |
| 0 | No |

There is another common encoding method: Target Encoding. This encoding scheme makes use of target variable, the variable that we want to predict, to transform each option of a categorical feature into a number. In general, we calcualtes a weighted mean, combining the mean for a particular option with the overall mean of the target.
$ Weighted Mean = \frac{n\times Option Mean + m\times Overall Mean}{n+m}$
with $n$ equal to the weight for Option Mean (the number of rows) and $m$ is the weight for Overall Mean (hyperparameter).

After Target Encoding...
| Country | LikeCoffee |
| :---: | :---: |
| 0.5 | Yes |
| 1 | Yes |
| 0.5 | No |
| 1 | Yes |
| 0.5 | Yes |
| 0.5 | No |
