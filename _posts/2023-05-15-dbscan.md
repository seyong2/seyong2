---
layout: post
title: DBSCAN
subtitle: A Clustering Technique You'd Better Know
gh-repo: seyong2
gh-badge: [star, fork, follow]
tags: [Statistics, Machine Learning, DBSCAN, Silhouette Score, K-Means, Hierarchical Clustering, K-Distance Graph]
comments: true
---

*This post is based on [StatQuest with Josh Starmer’s video about DBSCAN](https://www.youtube.com/watch?v=RDZUdRSDOok) and [How to Master the Popular DBSCAN Clustering Algorithm for Machine Learning by Analytics Vidhya](https://www.analyticsvidhya.com/blog/2020/09/how-dbscan-clustering-works/).*

DBSCAN stands for Density-Based Spatial Clustering of Applications with Noise and as its name suggests, we can tell it is a clustering algorithm based on the density of the data. Clustering is an unsupervised machine learning technique and its task is to group data by specific characteristics. K-means and hierarchical clustering algorithms are the most common clustering methods. However, these algorithms have limitations that make us turn to other clustering methods like DBSCAN which I will introduce in this post. The standard clustering algorithms are not good at identifying clusters of arbitrary shapes. For example, let us assume that we have a dataset that has the shape of two interleaving half circles. As we can see in the plot below, it is possible to infer that there are two possible clusters and some noises.

![data](https://github.com/seyong2/seyong2.github.io/blob/master/assets/img/figures_dbscan/data.png?raw=true)

What if we try to group the data into two using the K-means and hierarchical clustering techniques? The following plots show the results of the two algorithms. It is clear that they both fail to identify the two clusters as we expected. Some data points that belong to a circle are classified as if they were from the other circle. In addition, they are not capable of spotting outliers as all of the points are indicated as either purple or yellow. Since we have seen that the K-means and hierarchical clustering algorithms do not work well with data of arbitrary shapes, it is not hard to imagine their performance in high-dimensional data.

![k-means](https://github.com/seyong2/seyong2.github.io/blob/master/assets/img/figures_dbscan/kmeans.png?raw=true)

![hierarchical](https://github.com/seyong2/seyong2.github.io/blob/master/assets/img/figures_dbscan/hierarchical.png?raw=true)

One of the alternatives to the standard clustering methods that we’ve just seen now is DBSCAN and as I mentioned above, it forms clusters by the densities of the data. Then, data points that are tightly close to each other will be grouped into a cluster, and data in low-density regions will be classified as outliers. Another advantage of this method is that we do not need to specify beforehand the number of clusters, unlike K-means for example, which requires the number of centroids. The clustering process is as follows:

1. Starting with the unclustered data, we count how many points there are close to each point by drawing a circle around the point. To measure the distance between the points, Euclidean distance is the one commonly used but other distance measures can be considered. How large the circle will be, that is, the radius of the circle ($\epsilon$) is a hyperparameter that we have to set in advance.  A common approach to determine the optimal value of $\epsilon$ is to plot the K-distance graph and see at which point the curvature is the maximum.
2. We categorize each data point based on the number of close data points it has including itself. If a point is close at least to $x$ points, it is defined as a core point and if not, non-core (or border) point. Like $\epsilon$, the number of close points for a core point is also a hyperparameter. The value has to be at least the number of dimensions of the data plus one but generally, it is twice the dimensions. Domain knowledge also helps decide the optimal value.
3. A core point is randomly picked and assigned to the first cluster. Then, all the other core points that are adjacent to the first cluster are also included in the first cluster. The extension of the first cluster continues until there are no core points to be added. Note that border points are out of consideration.
4. Once done with this, include all of the non-core points that are close to core points in the cluster.
5. We, again, choose a core point that is not in the first cluster, form a second cluster, and repeat the same process to extend the cluster.
6. If there remain non-core points that do not belong to any of the clusters are classified as noise.

On the basis of the mechanism illustrated above, I tried to do clustering using DBSCAN on the same data. First, I set the minimum number of points  in a neighborhood for a point to be defined as a core point to be 4 because the data is two-dimensional. Then, I looked at the K-distance (4 in this case) graph to determine the radius of the circle. According to the graph, it seemed that at $\epsilon$ equal to 0.05, the curvature was the maximum. With this in mind, I proceeded with fitting the model and plotting the result to see how the DBSCAN grouped the data. However, the result was rather disappointing because it failed to distinguish one cluster from the other. Therefore, I decided to use another approach to determine the value of $\epsilon$, which is silhouette score.

![k-distance_graph](https://github.com/seyong2/seyong2.github.io/blob/master/assets/img/figures_dbscan/k_distance_graph.png?raw=true)

![dbscan1](https://github.com/seyong2/seyong2.github.io/blob/master/assets/img/figures_dbscan/dbscan_eps_0.05.png?raw=true)

The silhouette score is a metric to compute how good a clustering technique is. It calculates the average distance from each sample to other points in the same cluster ($a$) and the average distance from the same sample to points that belong to the nearest cluster ($b$). Then, for example, the silhouette score for sample $i$ can be defined as $\frac{b(i)-a(i)}{max(a(i),b(i))}$. We compute the mean score over all data points to measure the goodness of the clustering. Thus, ideally, we want to have a value of 1 because it means that the mean nearest-cluster distance is as large as possible and the mean intra-cluster distance is very close to 0. 

Then, assuming that the minimum number of data points for a sample to be a core point is 4, I computed the silhouette score over a range of values for $\epsilon$ (from 0.05 to 0.2 with step size of 0.01) to see at which value the score is the largest. According to the table above that shows the value of $\epsilon$ and its corresponding silhouette score, we conclude that when the radius of the circle is 0.11, the silhouette score reaches the maximum of 0.31. Then, plugging this value of 0.11 for $\epsilon$, if we fit the model again and plot the result as below, we can tell that the result looks promising, not like the one we had when $\epsilon$ was equal to 0.05. The model identified two clusters, yellow and green, and the points in purple are classified as outliers that do not belong to any of the two groups. I cannot figure out how the K-distance graph resulted in a value for $\epsilon$ that gave poor performance of the model. If you have any idea why, please let me know!

| $\epsilon$ | Silhouette Score |
| :---: | :---: |
| 0.05 | -0.71 |
| 0.06 | -0.58 |
| 0.07 | -0.30 |
| 0.08 | -0.13 |
| 0.09 | -0.04 |
| 0.10 | 0.05 |
| 0.11 | 0.31 |
| 0.12 | 0.15 |
| 0.13 | 0.16 |
| 0.14 | 0.17 |
| 0.15 | 0.22 |
| 0.16 | 0.21 |
| 0.17 | 0.24 |
| 0.18 | 0.25 |
| 0.19 | 0.17 |
| 0.20 | 0.17 |

![dbscan2](https://github.com/seyong2/seyong2.github.io/blob/master/assets/img/figures_dbscan/dbscan_eps_0.11.png?raw=true)


